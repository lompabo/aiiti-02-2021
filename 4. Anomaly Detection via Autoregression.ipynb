{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDE bandwidth: 0.03476971577055476\n",
      "Fitting a KDE estimator\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook setup\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from util import nab\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load data\n",
    "data_folder = '/app/data/nab'\n",
    "file_name = 'realKnownCause/nyc_taxi.csv'\n",
    "data, labels, windows = nab.load_series(file_name, data_folder)\n",
    "\n",
    "# Train and validation end\n",
    "train_end = pd.to_datetime('2014-10-24 00:00:00')\n",
    "val_end = pd.to_datetime('2014-12-10 00:00:00')\n",
    "\n",
    "# Cost model parameters\n",
    "c_alrm = 1 # Cost of investigating a false alarm\n",
    "c_missed = 10 # Cost of missing an anomaly\n",
    "c_late = 5 # Cost for late detection\n",
    "\n",
    "# Build a cost model\n",
    "cmodel = nab.ADSimpleCostModel(c_alrm, c_missed, c_late)\n",
    "\n",
    "# Compute the maximum over the training set\n",
    "trmax = data[data.index < train_end]['value'].max()\n",
    "# Normalize\n",
    "data['value'] = data['value'] / trmax\n",
    "# Separate the training data\n",
    "data_tr = data[data.index < train_end]\n",
    "\n",
    "# Apply a sliding window\n",
    "wlen = 48\n",
    "wdata = nab.sliding_window_1D(data, wlen=wlen)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# For a later comparison: train a KDE model\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Compute the bandhwidth\n",
    "q1 = data_tr['value'].quantile(0.25)\n",
    "q3 = data_tr['value'].quantile(0.75)\n",
    "sigma = data_tr['value'].std()\n",
    "m =  len(data_tr)\n",
    "h = 0.9 * min(sigma, (q3-q1) / 1.34) * m**(-0.2)\n",
    "print(f'KDE bandwidth: {h}')\n",
    "\n",
    "# Build and fit a density estimator\n",
    "print('Fitting a KDE estimator')\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=h)\n",
    "kde.fit(data_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Anomaly Detection via Autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Drawbacks\n",
    "\n",
    "**We managed to use KDE for anomaly detection to a good effect**\n",
    "\n",
    "But there are some drawbacks:\n",
    "\n",
    "* Training was pretty fast (except for the bandwidth optimization)\n",
    "* ...but obtaining predictions was _a bit slow_\n",
    "\n",
    "**Latency _may be an issue_:**\n",
    "\n",
    "* In our case, time steps are 30 minutes long\n",
    "* ...Which gives ample time to make predictions\n",
    "* In other problems, there are strict latency constraints\n",
    "  - E.g. graceful thermal throttling of a multi-core CPU\n",
    "  - E.g. speed control in an industrial pump\n",
    "\n",
    "**What if KDE turns up to be too slow?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Anomaly Detection via Autoregression\n",
    "\n",
    "**Let us see an alternative approach for anomaly detection:**\n",
    "\n",
    "The main idea is to build an _autoregressor_, say $f(x)$:\n",
    "\n",
    "* Autoregressor = a predictor for the next step in the time series\n",
    "* We can then use the _prediction error as an alarm signal_\n",
    "\n",
    "$$\n",
    "|f(x) - y| \\geq \\theta\n",
    "$$\n",
    "\n",
    "**Why doing that?**\n",
    "\n",
    "Regression is a classical Machine Learning task!\n",
    "\n",
    "* This trick allows us to use _any regression approach_ for anomaly detection\n",
    "* Linear Regression, (Ensemble of) Decision Trees, Neural Networks...\n",
    "\n",
    "**As usual, we will make an attempt with the simplest possible approach:**\n",
    "\n",
    "* I.e. we are going to use Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "**A few known things about Linear Regression**\n",
    "\n",
    "**1)** It's a _supervised_ learning approach\n",
    "\n",
    "* We need a training set $\\hat{x}$ and a target vector (tensor) $\\hat{y}$\n",
    "\n",
    "\n",
    "**2)** The goal is typically to fit a _linear function_, i.e.:\n",
    "\n",
    "$$\n",
    "f(x, w) = w_0 + \\sum_{j=1}^n w_i x_i\n",
    "$$\n",
    "\n",
    "* $w$ is a vector of $n+1$ weights, $w_0$ is used a constant and it's called _intercept_\n",
    "* If the signal to be predicted has 0-mean, $w_0$ can be omitted\n",
    "\n",
    "\n",
    "**3)** Training is done via the _Least Squares method_:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{argmin}_w \\|f(\\hat{x}, w) - \\hat{y}\\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "**A few less-known things about Linear Regression**\n",
    "\n",
    "\n",
    "**1)** The least squares method _can be optimally solved_ in polynomial time\n",
    "\n",
    "* ...Even by simple gradient descent!\n",
    "  - With gradient descent we get arbitrarily close to the optimum\n",
    "* It's a convex, unconstrained, numerical optimization problem\n",
    "\n",
    "\n",
    "**2)** The least squares method _works for non-linear functions_\n",
    "\n",
    "* In fact, you can fit any function in the form:\n",
    "$$\n",
    "f(x, w) = w_0 + \\sum_{j=1}^n w_i K(x)\n",
    "$$\n",
    "  - $K$ is called in this case a basis function and can be non-linear\n",
    "  - Any weighted sum of basis functions can be easily trained to optimality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "**A few less-known things about Linear Regression**\n",
    "\n",
    "In the least squares method we minimize:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m (f(\\hat{x}_i, w) - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\log \\left(\\prod_{i=1}^m e^{(f(\\hat{x}_i, w) - \\hat{y}_i)^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "**A few less-known things about Linear Regression**\n",
    "\n",
    "The we make a few additions:\n",
    "\n",
    "$$\n",
    "\\log \\left(\\prod_{i=1}^m \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{\\left(\\frac{f(\\hat{x}_i, w) - \\hat{y}_i}{\\sigma}\\right)^2} \\right)\n",
    "$$\n",
    "\n",
    "* As long as $\\sigma$ is constant, _the optimum is the same_\n",
    "\n",
    "This is the log likelihood of $\\hat{y}$!\n",
    "\n",
    "\n",
    "**3)** So the predictions can be interpreted as solutions of $\\text{argmax}_y P(\\hat{y} \\mid {\\bf x})$\n",
    "\n",
    "* I.e. the most likely values, assuming _a Normal distribution_\n",
    "* ...And a _fixed variance_\n",
    "\n",
    "This is an important (often) _hidden assumption_ of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preparing The  Dataset\n",
    "\n",
    "**Before using Linear Regression, we need to build the target vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp\n",
       "2014-07-02 00:00:00    0.440194\n",
       "2014-07-02 00:30:00    0.327429\n",
       "2014-07-02 01:00:00    0.249267\n",
       "2014-07-02 01:30:00    0.194811\n",
       "2014-07-02 02:00:00    0.158694\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdata_out = data.iloc[wlen:]['value']\n",
    "wdata_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The last known output will be at the end of the series\n",
    "* So we need to remove the last row from our sliding window dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdata_in = wdata.iloc[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In linear regression, all columns should have the same scale\n",
    "* This is true by construction in our case (nothing more to do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fitting the Linear Model\n",
    "\n",
    "**Next, we need to separate the training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdata_in_tr = wdata_in[wdata_in.index < train_end]\n",
    "wdata_out_tr = wdata_out[wdata_out.index <= train_end] # Notice the \"<=\" sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `wdata_in` and `wdata_out` have different indices (hence the \"<=\" sign)\n",
    "\n",
    "**Then, we can train our predictor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(wdata_in_tr, wdata_out_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predictions are associated to the `wdata_out` index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Obtaining the Predictions\n",
    "\n",
    "\n",
    "**We can now obtain the predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.01 ms, sys: 508 µs, total: 5.52 ms\n",
      "Wall time: 3.52 ms\n"
     ]
    }
   ],
   "source": [
    "%time pred_out = reg.predict(wdata_in)\n",
    "pred_out = pd.Series(index=wdata_out.index, data=pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The process is now very fast!\n",
    "* Time can be measures via the `%time` ipython magic\n",
    "\n",
    "For comparison, let us see the time for KDE (the _univariate_ estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.85 s, sys: 0 ns, total: 2.85 s\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "%time ldens = kde.score_samples(data)\n",
    "signal = pd.Series(index=data.index, data=-ldens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prediction  Quality\n",
    "\n",
    "**Let us have a look at the prediction quality:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c767e43d9ca24922a6e05aed62d95eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nab.plot_prediction_scatter(wdata_out, pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the R2 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789662792909525"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(wdata_out, pred_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Alarm Signal\n",
    "\n",
    "**We now just need to compute the errors to obtain our signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeea579d8264c71a5b82b373fa62595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = wdata_out - pred_out\n",
    "signal = np.abs(err)\n",
    "nab.plot_series(signal, labels, windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It does not seem to be particularly good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Threshold Optimization\n",
    "\n",
    "**We can proceed as usual, by optimizing the threshold and checking the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.2909090909090909, corresponding cost: 15\n"
     ]
    }
   ],
   "source": [
    "thr_range = np.linspace(0.1, 1, 100)\n",
    "signal_opt = signal[signal.index < val_end]\n",
    "labels_opt = labels[labels < val_end]\n",
    "windows_opt = windows[windows['end'] < val_end]\n",
    "best_thr, best_cost = nab.opt_thr(signal_opt, labels_opt,\n",
    "                                  windows_opt,  cmodel, thr_range)\n",
    "print(f'Best threshold: {best_thr}, corresponding cost: {best_cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over all the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on the whole dataset 35\n"
     ]
    }
   ],
   "source": [
    "ctst = cmodel.cost(signal, labels, windows, best_thr)\n",
    "print(f'Cost on the whole dataset {ctst}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Considerations\n",
    "\n",
    "**Some considerations and take-home messages**\n",
    "\n",
    "An alternative way to perform Anomaly Detection:\n",
    "\n",
    "* Train an autoregressor and use the (absolute) error as an alarm signal\n",
    "* You can use any regression approach\n",
    "* Typicallly faster signal generations\n",
    "\n",
    "We are not restricted to absolute errors:\n",
    "\n",
    "* We can use other functions, with different results\n",
    "* A better interpretation will come in a few lectures\n",
    "\n",
    "The least squares method:\n",
    "\n",
    "* Assumes the predictions is Normally distributed with _fixed_ variance\n",
    "* The fixed variance is an important limitation\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
